---
title: "Validação CAAS - Nayara"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.



####### Preparando o ambiente e instalando os pacotes

```{r}
install.packages("VIM")
install.packages("VIMGUI")
install.packages("gWidgetsRGtk2")
install.packages("mi")
install.packages("epicalc")
install.packages("sem")
install.packages("ggplot2")
install.packages("psych")
install.packages("RCurl")
install.packages("irr")
install.packages("nortest")
install.packages("moments")
install.packages("GPArotation")
install.packages("nFactors")
install.packages("boot")
install.packages("psy")
install.packages("car")
install.packages("vcd")
install.packages("gridExtra")
install.packages("gdata")
install.packages("sqldf")
install.packages("reshape2")
install.packages("mclust")
install.packages("foreign")
install.packages("survival")
install.packages("memisc")
install.packages("lme4")
install.packages("lmerTest")
install.packages("dplyr")
install.packages("QCA")
install.packages("VennDiagram")
install.packages("qgraph")
install.packages("igraph")
install.packages("ltm")
install.packages("gmodels")
install.packages("eRm")
install.packages("mirt")
install.packages("dplyr")
install.packages("devtools")
install.packages("reshape")
install.packages("readxl")
install.packages("openxlsx")
install.packages("Hmisc")
```

```{r}
library("VIM")
library("VIMGUI")
library("gWidgetsRGtk2")
library("mi")
library("epicalc")
library("sem")
library("ggplot2")
library("psych")
library("RCurl")
library("irr")
library("nortest")
library("moments")
library("GPArotation")
library("nFactors")
library("boot")
library("psy")
library("car")
library("vcd")
library("gridExtra")
library("gdata")
library("sqldf")
library("reshape2")
library("mclust")
library("foreign")
library("survival")
library("memisc")
library("lme4")
library("lmerTest")
library("dplyr")
library("QCA")
library("VennDiagram")
library("qgraph")
library("igraph")
library("ltm")
library("gmodels")
library("eRm")
library("mirt")
library("dplyr")
library("devtools")
library("reshape")
library("readxl")
library("openxlsx")
library("Hmisc")
``


######################################################
### Importando o banco de dados

```{r}
library(readxl)
dataCAAS <- read_excel("C:/Users/nayar/Dropbox/Doutorado/Projeto Doutorado/Projeto/Estudo 2 - Validação CAAS/Validação AFC/ImpCAAS_treinador_atleta.xlsx")
View(dataCAAS)
```



#####################################################
### RANDOMIZAR OS DADOS#

```{r}
dadosrandomizados <- dataCAAS
View(dadosrandomizados)

dadosrandomizados[sample(nrow(dadosrandomizados), 400),]
CAASAFE <- dadosrandomizados[sample(nrow(dadosrandomizados), 400),]
View(CAASAFE)
```



#############################################################
#Descriptives
#############################################################
#BASIC DESCRIPTIVES and EXPLORATORY ANALYSIS
#############################################################
#Section wih several exploratory data analysis functions
###### Exploratory Data Anlysis
###### UNIVARIATE

```{r}
library(pastecs)
dim(dataCAAS)
str(dataCAAS)
head(dataCAAS)
names(dataCAAS)
sapply(dataCAAS, mean, na.rm=TRUE)
stat.desc(dataCAAS, basic=TRUE, desc=TRUE, norm=FALSE, p=0.95)
with(dataCAAS,by(dataCAAS, ad.test))
stat.desc(dataCAAS)
#with (dataCAAS, t.test(dataCAAS, Idade))
#with(dataCAAS,by(dataCAAS,outcome,ad.test)) # Anderson-Darling test for normality
#skewness(dataCAAS) #Will provide skweness analysis
#kurtosis(dataCAAS$Idade) - 3 #Will provide kurtosis analysis
#qplot(dataCAAS$Idade) # histogram plot
```



### Numerical descriptives

```{r}
library(Hmisc)
summary(dataCAAS) #This comand will provide a whole set of descriptive #results for each variables
describe(dataCAAS)
#with(dataCAAS,by(dataCAAS,outcome,describe))
#with(dataCAAS,by(dataCAAS,outcome,summary))
```



##### BIVARIATE

```{r}
#Graphing and homogeneity
boxplot(dataCAAS) #will provide a boxplot for the #variables to analysis potential outliers
## Bartlett Test of Homogeneity of Variances
bartlett.test(dataCAAS$AP1~dataCAAS$AP19) ## errado, não sei interpretar o valor gerado
library(psych)
cortest.bartlett(cor_data, n= 689, diag = TRUE) ## correto = usar esse
## Figner-Killeen Test of Homogeneity of Variances
fligner.test(dataCAAS$AP1~dataCAAS$AP19)
install.packages("lawstat")
library(lawstat)
levene.test(dataCAAS$AP1~dataCAAS$AP19) ## Não rodou
```



##############################################################
### NETWORK 
##############################################################

```{r}
# # Define the amout of factor to retain
#Group of functinos to determine the number os items to be extracted
library(qgraph)
cor_data<-cor_auto(dataCAAS[,-c(1:11,31:44)]) ### sem dimensões e variáveis independentes
```


### Caso precise de alterações no banco
cor_data<-cor_auto(data02[,-c(1,2)]) ### com dimensões
cor_data<-cor_auto(data02[,-c(1,2,3,6,7,10,12,16,17,22,27,28,29,30,31,32)]) ### sem itens negativos e 1, 4, 8,14


```{r}
library(igraph)

### Community analysis
comprehension_network_glasso<-qgraph(cor_data,
layout="spring",
vsize=6,esize=20,graph="glasso",
sampleSize=nrow(dataCAAS),
legend.cex = 2.0,GLratio=1.5,minimum=0.1, threshold = TRUE)
```


```{r}
##Calculating Community measures
library(intergraph)
library(igraph)
library(qgraph)
g <- as.igraph(comprehension_network_glasso) #creating igraph object
h<-walktrap.community(g) #creatin community object
h<-spinglass.community(g, weights=NA)
plot(h,g) #plotting community network
h$membership #extracting community membership for each node on the network
community<-data.frame(h$membership,rownames(cor_data))
```



#listing grouping variables in the network resulting from the community analysis
```{r}
network_groups<-list(
Component1=as.numeric(rownames(community)[community[,1]==1]),
Component2=as.numeric(rownames(community)[community[,1]==2]),
Component3=as.numeric(rownames(community)[community[,1]==3]),
Component4=as.numeric(rownames(community)[community[,1]==4]))

network_groups<-list(
  Component1=c(12,13,14,15,16,17,18),
  Component2=c(19,20,21,22,23,24,25),
  Component3=c(26,27,28,29,30))
```


### Building network figures 
# 3 types are created to get an avarege position and layout

```{r}
#GLASSO NETWORK
network_glasso<-qgraph(cor_data,layout="spring",
 	vsize=6,esize=20,graph="glasso",
 	sampleSize=nrow(dataCAAS),
 	legend.cex = 0.5,GLratio=1.5)
```

```{r}
#PARTIAL CORRELATION NETWORK
network_pcor<-qgraph(cor_data,layout="spring",
vsize=6,esize=20,graph="pcor",threshold="holm",
sampleSize=nrow(dataCAAS),
legend.cex = 0.5,GLratio=1.5)
network_pcor
```


```{r}
#CORRELATION NETWORK
network_cor<-qgraph(cor_data,layout="spring",
vsize=6,esize=20,legend.cex = 0.5,GLratio=1.5)
layout1<-averageLayout(network_glasso,network_pcor,network_cor)
```



```{r}
library(qgraph)
centralityPlot(network_cor) 
```

```{r}
install.packages("bootnet")
library(bootnet)
EdgeWgt<- bootnet(network_cor, nBoots=19)
summary(EdgeWgt) 
plot(EdgeWgt, labels=TRUE, order="sample") 
```




## Não rodou

```{r}
final_importance_network <-qgraph (cor_data,
                                 esize=15,graph="glasso",
                                 sampleSize=nrow(dataCAAS),
                                 legend.cex = 0.6,cut = 0.3, maximum = 1, 
                                 minimum = 0.1, esize = 20,vsize = 5, 
                                  groups=network_groups, threshold = TRUE,
                                 color=c("gold"),borders = FALSE,
                                 labels=FALSE) #,gray=T,)#,nodeNames=nomesqsg,layoutScale=c(2,2)
dev.off()
                                                                                               
```



### Gerar heatmap para demonstração da matriz de correlação ##################

```{r}
##### com todos os itens
cor_data<-cor_auto(dataCAAS[,-c(1:11,31:44)])
cor(cor_data)
library(reshape2)
cormat <- round(cor_data,2)
head(cormat)

melted_cormat <- melt(cormat)
head(melted_cormat)
library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()

get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

upper_tri <- get_upper_tri(cormat)
upper_tri
```

###### Heatmap
```{r}
library(ggplot2)
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "green", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Items\nCorrelation") +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 12, hjust = 1))+
  coord_fixed()
```



###### Modelo final 
```{r}
cor(cor_data)
library(reshape2)
cormat <- round(cor(cor_data),2)
head(cormat)

melted_cormat <- melt(cormat)
head(melted_cormat)
library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()

get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

upper_tri <- get_upper_tri(cormat)
upper_tri

```


## OU - caso queira a matriz virada
```{r}
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}
lower_tri <- get_lower_tri(cormat)
lower_tri


library(reshape2)
melted_cormat <- melt(upper_tri, na.rm = TRUE)

```



############################################################
#ANALISE PARALELA E EIGEN VALUES

```{r}
#MODEL 1 - Risk due to road deisgn
model1_CAAS <-with(dataCAAS,data.frame(AP1,AP2,AP3,AP4,AP5,AP6,AP7,AP8,AP9,AP10,AP11,                                    AP12,AP13,AP14,AP15,AP16,AP17,AP18,AP19))

cor_data<-cor_auto(model1_CAAS)
cor_data
```



### Function to calculate the KMO values

```{r}
kmo = function(cor_data){
	
	library(MASS)
	X <- cor(as.matrix(cor_data))
	iX <- ginv(X)
	S2 <- diag(diag((iX^-1)))
	AIS <- S2%*%iX%*%S2                      # anti-image covariance matrix
	IS <- X+AIS-2*S2                         # image covariance matrix
	Dai <- sqrt(diag(diag(AIS)))
	IR <- ginv(Dai)%*%IS%*%ginv(Dai)         # image correlation matrix
	AIR <- ginv(Dai)%*%AIS%*%ginv(Dai)       # anti-image correlation matrix
	a <- apply((AIR - diag(diag(AIR)))^2, 2, sum)
	AA <- sum(a)
	b <- apply((X - diag(nrow(X)))^2, 2, sum)
	BB <- sum(b)
	MSA <- b/(b+a)                        # indiv. measures of sampling adequacy
	
	AIR <- AIR-diag(nrow(AIR))+diag(MSA)  # Examine the anti-image of the
	# correlation matrix. That is the
	# negative of the partial correlations,
	# partialling out all other variables.
	
	kmo <- BB/(AA+BB)                     # overall KMO statistic
	
	# Reporting the conclusion
	if (kmo >= 0.00 && kmo < 0.50){
		test <- 'The KMO test yields a degree of common variance
		unacceptable for FA.'
	} else if (kmo >= 0.50 && kmo < 0.60){
		test <- 'The KMO test yields a degree of common variance miserable.'
	} else if (kmo >= 0.60 && kmo < 0.70){
		test <- 'The KMO test yields a degree of common variance mediocre.'
	} else if (kmo >= 0.70 && kmo < 0.80){
		test <- 'The KMO test yields a degree of common variance middling.'
	} else if (kmo >= 0.80 && kmo < 0.90){
		test <- 'The KMO test yields a degree of common variance meritorious.'
	} else {
		test <- 'The KMO test yields a degree of common variance marvelous.'
	}
	
	ans <- list(  overall = kmo,
								report = test,
								individual = MSA,
								AIS = AIS,
								AIR = AIR )
	return(ans)
	
}    
kmo(cor_data)# end of kmo()
```



```{r}
install.packages("nFactors")
library(nFactors)
par(mfrow=c(2,2)) #Command to configure the plot area for the scree plot graph
ev <- eigen(cor_data) # get eigenvalues - insert the data you want to calculate the scree plot for
ev # Show eigend values
ap <- parallel(subject=nrow(cor_data),var=ncol(cor_data),rep=100,cent=.05) #Calculate the acceleration factor
summary(ap)
nS <- nScree(ev$values) #Set up the Scree Plot 
plotnScree(nS) # Plot the ScreePlot Graph
my.vss <- VSS(cor_data,title="CAAS data")
print(my.vss[,1:3],digits =2)
VSS.plot(my.vss, title="CAAS data.")
scree(cor_data)
VSS.scree(cor_data)
fa.parallel(cor_data,n.obs=689)
```



##############################################################
#PCA
#############################################################
# Pricipal Components Analysis
# entering raw data and extracting PCs 
# from the correlation matrix 

```{r}
fit <- psych::principal(cor_data,nfactors=3,rotate="none",scores=TRUE)
fit
summary(fit) # print variance accounted for 
loadings(fit) # pc loadings 
fit$scores
pca1<-predict(fit,model1_CAAS)
scores<-scoreItems(fit$weights,dataCAAS[,-1],totals=TRUE)
summary(scores)
describe(scores$scores)
```

## Falta terminar esse código
```{r}
model <- principal(cor_data, nfactors=3, rotate='none', scores=T, cov=T)
L <- model$loadings            # Just get the loadings matrix
S <- model$scores              # This gives an incorrect answer in the current version

d <- model1_CAAS              # get your data
dc <- scale(d,scale=FALSE)     # center the data but do not standardize it
pca1 <- dc %*% L                 # scores are the centered data times the loadings
lowerCor(sc)                   #These scores, being principal components
model
#                                # should be orthogonal 
```



##############################################################
#EXPLORATORY FACTOR ANALYSIS
#############################################################
#Functino to exctract the factor loadings. 

```{r}
### Holds of estimations or rotations - não sei se precisa rodar isso

fa_model<-fa(cor_data,3,fm="uls",rotate="oblimin")
fa_model
fa(cor_data,3,fm="pa",rotate="promax")
fa
fa(cor_data,3,fm="pa",rotate="oblimin")

```


```{r}
## Just to make sure all codes will run
install.packages("EFAutilities")
install.packages("polycor")
install.packages("DescTools")
install.packages("GPArotarion")
library(psych)
library(EFAutilities)
library(polycor)
library(DescTools)
library(GPArotation)

### Based on a polychoric correlation matrix
fa(cor_data,3,fm="uls",rotate="oblimin")
threefactor <- fa(cor_data,nfactors = 3,rotate = "oblimin",fm="uls")
print(threefactor)
print(threefactor$loadings,cutoff = 0.3)
fa.diagram(threefactor)
```


##############################################################
#CONFIRMATORY FACTOR ANALYSIS
#############################################################

### Modelo com 3 fatores

```{r}

#install.packages("lavaan")
library(lavaan)

threefactors_model <- 'ANS =~ AP1 + AP2 + AP3 + AP4 + AP5 + AP6 
			 EVT =~ AP8 + AP9 + AP10 + AP11 + AP12 + AP13 + AP14
			 SEG =~ AP15 + AP16 + AP17 + AP18 + AP19'

fit <- lavaan::cfa(threefactors_model, data = dataCAAS,
                   estimator="WLSMV",
                   ordered=names(cor_data)
                   )
summary(fit, fit.measures=TRUE)
fitMeasures(fit, fit.measures = "all", baseline.model = NULL)
parameterEstimates(fit)
Est <- parameterEstimates(fit, ci = TRUE, standardized = TRUE)
subset(Est, op == "=~")

### Modification Indexes
Mod <- modificationIndices(fit)
subset(Mod, mi > 10)
Mod

```


```{r}
library(semPlot)
semPlotModel(fit)
semPaths(fit, "std")
semPaths(fit, "std", title = TRUE, edge.label.cex = 0.8, edge.color = "black", curvePivot = TRUE, 
         layout = "tree3", cardinal = "lat cov", width = 8, rotation = 2, intercepts = TRUE,  residScale = 10,
         sizeMan = 4,  sizeLat = 7,  height = 5,   mar = c(3,18,7,20), style = "lisrel", what = "paths")
```

```{r}
install.packages("lavaan")
library(lavaan)
lavaanPlot(model = fit, edge_options = list(color = "grey"))
lavaanPlot(model = fit, labels = labels, node_options = list(shape = "box", fontname = "Helvetica"), edge_options = list(color = "grey"), coefs = TRUE, stand = FALSE)
```


#### EXEMPLO ##########
semPaths(fit, "std",
         what = "col", # this argument controls what the color of edges represent. In this case, standardized parameters
         whatLabels = "name", # This argument controls what the edge labels represent. In this case, parameter estimates
         as.expression = c("nodes","edges"), # This argument draws the node and edge labels as mathematical exprssions
         style = "lisrel", # This will plot residuals as arrows, closer to what we use in class
         residScale = 10, # This makes the residuals larger
         layout = "tree2", # tree layout options are "tree", "tree2", and "tree3"
         cardinal = "lat cov", # This makes the latent covariances connet at a cardinal center point
         curvePivot = TRUE, # Changes curve into rounded straight lines
         sizeMan = 4, # Size of manifest variables
         sizeLat = 10, # Size of latent varibales
         edge.label.cex = 1,
         mar = c(9,1,8,1), # Sets the margins
         reorder = FALSE, # Prevents re-ordering of ovbserved variables
         width = 8, # Width of the plot
         height = 5, # Height of plot
         borders = FALSE) # Disable borders

```{r}
install.packages("semPlot")
install.packages("rgbif")
library(ggplot2)
library(rgbif)
```



#RELIABILITY
##############################################################
### INTERNAL CONSISTENCY

#RELIABILITY
```{r}
psych::alpha(cor_data,n.iter=1000,check.keys=TRUE)
```


#Alpha dimensão Ansioso
```{r}
cor_dataF1<-cor_auto(dataCAAS[,c(12,13,14,15,16,17,18)]) 
psych::alpha(cor_dataF1,n.iter=1000,check.keys=TRUE)
```


#Alpha dimensão Evitativo
```{r}
cor_dataF2<-cor_auto(dataCAAS[,c(19,20,21,22,23,24,25)]) 
psych::alpha(cor_dataF2,n.iter=1000,check.keys=TRUE)
```


#Alpha dimensão Seguro
```{r}
cor_dataF3<-cor_auto(dataCAAS[,c(26,27,28,29,30)]) 
psych::alpha(cor_dataF3,n.iter=1000,check.keys=TRUE)
```




### Modification Indexes
```{r}
Mod <- modificationIndices(fit)
subset(Mod, mi > 10)
```


#Composite Reliabilty
```{r}
sum(Est[1:4,4])^2/(sum(Est[1:4,4])^2+sum(Est[17:20,4])) ### Fator 1 - mudar os valores
sum(Est[5:9,4])^2/(sum(Est[5:9,4])^2+sum(Est[21:25,4])) ### Fator 2 - mudar os valores
sum(Est[10:13,4])^2/(sum(Est[10:13,4])^2+sum(Est[26:29,4])) ### Fator 3 - mudar os valores
```









